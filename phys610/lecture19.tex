\section{Canonical Transformations}
In Hamiltonian formalism,
$p$ and $q$ are on an equal footing.

What that means is we can make coordinate transformations that mix up the $p$
and $q$.
\begin{align}
    q_i &\to Q_i = Q_i\left( q, p, t  \right)\\
    p_i &\to P_i = P_i\left( q, p, t  \right)
\end{align}
Now in the Hamiltonian formalism,
$p$ and $q$ are on an equal footing.
So in general you can think of a transformation that mixes the $p$ and $q$.
But there's a catch.
Not all such transformations are allowed.

Only those transformations that are allowed are those that leave Hamiltonian's
equations invariant,
so that in general
\begin{align}
    H(q, p) \to K(Q, P, t)
\end{align}
and what we need is for
\begin{align}
    \frac{\partial K}{\partial P_i} &= \dot{Q}_i
\end{align}
and
\begin{align}
    \frac{\partial K}{\partial Q_i} &= -\dot{P}_i
\end{align}
so you're allowed a much bigger set of transformations,
but the condition is that they still have to respect Hamilton's equations.

\begin{question}
    How are $q$ and $\dot{q}$ not on an equal footing in the Lagrangian
    formalism?
\end{question}
If you know $q(t)$,
you know $\dot{q}$ from the derivative.
You have second order differential equation for $q(t)$.
Here,
you get two first-order equations for $p$ and $q$ separately.
You can vary $p$ and $q$ independently to get the equations.
But you can't do that in the Lagrangian formalism.

So anyway,
they shouldn't change the form of Hamilton's equations.

Transformations that satisfy this condition are called canonical
transformations.
This is the set of allowed transformations that leave the form of Hamilton's
equations invariant.

Unfortunately,
this is a huge topic,
quite a complicated one,
and we don't have enough time to go into great detail.
So I will look at a subset of these transformations.
I'm going to look at descriptive canonical transformations.
These are the most important subset,
but if you look in the book,
you'll see a more extended discussion.
The real difference is mostly the ones we're looking at are not going to
involve time.
So if you have transformations that mix $p$ and $q$ that don't mix time.
For those,
there's a simplification that $H=K$.
That's not very clear,
but it's part of the bigger formalism.

\section{Restricted Canonical Transformations}
So they are
\begin{align}
   q_i &\to Q_i(q, p)\\
   p_i &\to P_i(q, p)
\end{align}
and the $H$ and $K$ are the same
\begin{align}
    H(q, p) = K(Q, P)
\end{align}
This is what defines the restricted set of canonical transformations.
What we do is,
we begin by writing Hamilton's equations in a symmetric form.
So you put the $p$'s and $q$'s into a single vector.
\begin{align}
    \vec{z} &=
    \begin{pmatrix}
        q_1\\
        q_2\\
        \vdots\\
        q_n\\
        p_1\\
        p_2\\
        \vdots\\
        p_n
    \end{pmatrix}
\end{align}
So we just defined this giant vector $2n$ components
with all the $p$'s and the $q$'s together.

And then there's a standard matrix.
We introduce a $2n\times 2n$ matrix called $\hat{J}$,
which is of this form
\begin{align}
    \hat{J} &=
    \begin{pmatrix}
        0_{n\times n} & I_{n\times n}\\
        - I_{n\times n} & 0_{n\times n}\\
    \end{pmatrix}
\end{align}
Where the $0_{n\times n}$ denotes an $n\times n$ matrix of zeros,
and $1_{n\times n}$ is the $n\times n$ identity matrix.
It has the funny property that if you take the transpose you get $-1$ of itself.

In this notation,
Hamilton's equations are
\begin{align}
    \dot{\vec{z}} &=
    \hat{J}
    \frac{\partial H}{\partial \vec{z}}
\end{align}
or alternatively
\begin{align}
    \dot{z}_i
    &=
    \sum_{j}
    \hat{J}_{ij}
    \frac{\partial H}{\partial z_j}
\end{align}
This is just a rewrite.

So now we make our transformation
from $z=(q_i, p_i)$ to $w=(Q_i, P_i)$.
In other word's we're going from
\begin{align}
    z_i &\to w_i = w_i(z)
\end{align}
Let's keep going.

So now these $w$'s must satisfy an equation just like the equation for the
$\dot{z}$.
That is,
\begin{align}
    \dot{w}_i &=
    \sum_{j}
    \frac{\partial w_i}{\partial z_j}
    \dot{z_j}
\end{align}
but this $\dot{z}_j$ satisfies
\begin{align}
    \dot{z}_j &=
    \sum_{k}
    \hat{J}_{jk}
    \frac{\partial H}{\partial z_k}
\end{align}
so then the equation becomes
\begin{align}
    \dot{w} &=
    \sum_{j,j}
    \frac{\partial w_i}{\partial z_j}
    \hat{J}_{jk}
    \frac{\partial H}{\partial z_k}
\end{align}
but this last derivative can be written
\begin{align}
    \frac{\partial H}{\partial z_k}
    &=
    \sum_{l}
    \frac{\partial H}{\partial w_l}
    \frac{\partial w_l}{\partial z_k}
\end{align}
so putting it together
\begin{align}
    \dot{w}_i &=
    \sum_{l}
    \left( 
    \sum_{jk}
    \frac{\partial w_i}{\partial z_j}
    \hat{J}_{jk}
    \frac{\partial w_l}{\partial z_k}
    \right)
    \frac{\partial H}{\partial w_l}
\end{align}
and you'll see why I put the brackets like that in a minute.
So this expression in the brackets is
\begin{align}
    \sum_{jk}
    \frac{\partial w_i}{\partial z_j}
    \hat{J}_{jk}
    \frac{\partial w_l}{\partial z_k}
    &=
    \left( J \hat{J} J \right)_{il}
\end{align}
where $J$ here is the Jacobian matrix of the transformation.
So when we transform from the $z$'s to the $w$'s,
there's a Jacobian matrix associated with that transformation.
So the Jacobian matrix is the matrix whose elements are
\begin{align}
    J_{ij} &=
    \frac{\partial w_i}{\partial z_j}
\end{align}
Those are the elements of the Jacobian matrix.
And you can see that with this definition,
this thing in the brackets is indeed just $\left( J \hat{J} J \right)_{il}$.

So then this equation can then be written as
\begin{align}
    \dot{\vec{w}}
    &=
    \left(
    J \hat{J} J^T
    \right)
    \frac{\partial H}{\partial \vec{w}}
\end{align}
So this equation comes once
once you recognize that expression can be written in terms of the Jacobian.
The condition for the transformation to be canonicial is tht this equation
should have the same form as this equation
\begin{align}
    \dot{\vec{z}} &=
    \hat{J}
    \frac{\partial H}{\partial \vec{z}}
\end{align}
That is, 
the equation for $\dot{w}$ should have the same form as the equation for
$\dot{z}$.

So the condition for Hamilton's equations to take the same form is that
\begin{align}
    \boxed{J \hat{J} J^T = \hat{J}}
\end{align}
If this condition is satisfied then we are done.
This is the condition you have to remember.
It's easy in the exam to get a question that asks to show that a transformation
is canonical.
Then you just calculate the Jacobian matrix,
then compute this to check that you get $\hat{J}$ back.

If this condition holds,
then we say that the Jacobian is \emph{symplectic}.
It's a technical term.

If you study group theory,
you'll find that matrices that satisfy this equation forms a group,
called the symplectic group.
If you study classical mechanics in detail,
a large part of it is understanding properties of the symplectic group.
This is one of the fundamental classical continuous groups.

\begin{question}
    Is this the same as the canonical group?
\end{question}
No. Symplectic is not the same as canonical.
This symplectic group is the real symplectic group.
It's called $SP_2(i)$.
But in quantum theory,
the transformation also has to be unitary,
and it's complex.
For example, in QM,
you have an extra condition of unitary,
and you're allowed to have a complex map.
So things can be symplectic without being canonical.

This $\hat{J}$ matrix shows up everywhere beyond classical mechanics
in the mathematics literature.
Unfortunately,
this is what everyone calls it $J$,
without the hat,
but unfortunately,
the Jacobian is also called $J$,
so I have to choose which one has the hat.

So we found this equation.
We now show that this condition is equivalent to the requirement that the new
coordinates satisfy
\begin{align}
    \left\{ 
    Q_i, Q_j
    \right\}
    &=
    \left\{ 
    P_i, P_j
    \right\}
    = 0\\
    \left\{ 
    Q_i, P_i
    \right\}
    &=
    \delta_{ij}
\end{align}
The claim is,
that if the new coordinates satsify the Poisson brackets,
then you are guaranteed that symplectic condition.

So let's show that.
This is just an exercise in linear algebra.
The Jacobian matrix by definition is
\begin{align}
    \frac{\partial w_i}{\partial z_j} &=
    J_{ij}\\
    &=
    \begin{pmatrix}
        \partial Q_i/ \partial q_j & \partial Q_i / \partial p_j\\
        \partial P_i/ \partial q_j & \partial P_i / \partial p_j\\
    \end{pmatrix}
\end{align}
What we're doing is evaluating this condition,
but in terms of the $Q$'s and $P$'s rather than in $z$-space.
Just blindly plugging it in.
Then what we have is
\begin{align}
    \left( J \hat{J} J^T \right)_{ik} &=
    \sum_{j} \sum_{k}
    \frac{\partial w_i}{\partial z_j}
    \hat{J}_{jk}
    \frac{\partial w_i}{\partial z_k}\\
    &=
    \begin{pmatrix}
        \frac{\partial Q_i}{\partial q_j} &
        \frac{\partial Q_i}{\partial p_j}\\
        \frac{\partial P_i}{\partial q_j} &
        \frac{\partial P_i}{\partial p_j}\\
    \end{pmatrix}
    \begin{pmatrix}
        0 & \delta_{ij}\\
        - \delta_{jk} & 0
    \end{pmatrix}
    \begin{pmatrix}
        \frac{\partial Q_i}{\partial q_k} &
        \frac{\partial P_l}{\partial q_k}\\
        \frac{\partial Q_l}{\partial p_k} &
        \frac{\partial P_l}{\partial p_k}
    \end{pmatrix}
\end{align}
and as an exercise in linear algebra,
if you multiply this out,
you get
\begin{align}
    \begin{pmatrix}
        0 & \delta_{ij}\\
        - \delta_{jk} & 0
    \end{pmatrix}
    \begin{pmatrix}
        \frac{\partial Q_i}{\partial q_k} &
        \frac{\partial P_l}{\partial q_k}\\
        \frac{\partial Q_l}{\partial p_k} &
        \frac{\partial P_l}{\partial p_k}
    \end{pmatrix}
    &=
    \begin{pmatrix}
        \frac{\partial Q_i}{\partial p_j} &
        \frac{\partial P_l}{\partial p_j}\\
        -\frac{\partial Q_l}{\partial q_j} &
        -\frac{\partial P_l}{\partial q_j}
    \end{pmatrix}
\end{align}
If you do the multiplication,
I claim that this is what you're going to find.
\begin{align}
    \left(J \hat{J} J\right)_{ik}
    &=
    \begin{pmatrix}
        \left\{ Q_i, Q_l \right\} &
        \left\{ Q_i, P_l \right\}\\
        \left\{ P_i, Q_l \right\} &
        \left\{ P_i, p_l \right\}
    \end{pmatrix}
\end{align}
and so if this is really equal to $\hat{J}_{il}$,
then we get the conditions
\begin{align}
    \left\{ Q_i, Q_l \right\} &=
    \left\{ P_i, P_l \right\} = 0
\end{align}
and
\begin{align}
    \left\{ Q_i, P_l \right\} &= \delta_{il}
\end{align}

I just want you to convince yourself that the $ij$-th element,
corresponds to that block.
What it is depends on which block you're in.

\begin{question}
    If we actually set out to compute the Jacobian,
    we could just do it element by element right?
\end{question}
I just wanted to show that the condition
$J \hat{J} J^T = \hat{J}$
is equivalent to
$\left\{ Q_i, Q_l \right\} = \left\{ P_i, P_l \right\} = 0$
and
$\left\{ Q_i, P_l \right\} = \delta_{il}$.
The two are completely equivalent mathematically.
In the exam you could show either.

\section{Poisson Bracket Invariance}
There's one more thing I want to show.
The Poisson bracket is invariant under canonical transformation.
Let me explain what I mean by this.
Suppose you have a transformation
\begin{align}
    q &\to Q(q, p)\\
    p &\to P(q, p)
\end{align}
And suppose you have a Poisson bracket that is by definition,
what I mean is that they are the same in the old coordinates and the new
coordinates.
\begin{align}
    \left\{ f, g \right\}
    &=
    \sum_{i}
    \left( 
    \frac{\partial f}{\partial q_i}
    \frac{\partial g}{\partial p_i}
    -
    \frac{\partial g}{\partial q_i}
    \frac{\partial f}{\partial p_i}
    \right)
    \sum_{i}
    \left( 
    \frac{\partial f}{\partial Q_i}
    \frac{\partial g}{\partial P_i}
    -
    \frac{\partial g}{\partial Q_i}
    \frac{\partial f}{\partial P_i}
    \right)
\end{align}
That means,
if you have a Poisson bracket,
you can evalaute it in any coordinates you like,
provided they are related by canonical transformation.

Let's prove this.
This is one of those things where it proves useful to be good at linear algebra.
To prove this note that
\begin{align}
    \left\{ f, g \right\}
    &=
    \sum_i
    \left( 
    \frac{\partial f}{\partial q_i}
    \frac{\partial g}{\partial p_i}
    -
    \frac{\partial f}{\partial p_i}
    \frac{\partial g}{\partial q_i}
    \right)\\
    &=
    \sum_{i}
    \sum_{j}
    \left( 
    \frac{\partial f}{\partial z_i}
    \hat{J}_{ij}
    \frac{\partial g}{\partial z_j}
    \right)
\end{align}
So we're going to expand it out and show the claim that they are equal.

Under $z\to w(z)$,
we have
\begin{align}
    \frac{\partial f}{\partial z_i}
    &=
    \sum_{j}
    \left( \frac{\partial f}{\partial w_j} \right)
    \underbrace{\left( \frac{\partial w_j}{\partial z_j} \right)}_{J_{ji}}
\end{align}
where we notice the second factor is just the Jacobian matrix element.

Then we have
\begin{align}
    \left\{ f, g \right\}
    &=
    \sum_i \sum_j \sum_k \sum_l
    \frac{\partial f}{\partial w_k}
    J_{ki}
    \hat{J}_{ij}
    J_{lj}
    \frac{\partial g}{\partial w_l}\\
    &=
    \sum_k \sum_l
    \frac{\partial f}{\partial w_k}
    \left( J \hat{J} J^T \right)_{kl}
    \frac{\partial g}{\partial w_l}
\end{align}
But because the transformation is canonical,
\begin{align}
    J \hat{J} J^T &= \hat{J}_{kl}
\end{align}
we find
\begin{align}
    \left\{ f, g \right\}
    &=
    \sum_l
    \frac{\partial f}{\partial w_k}
    \hat{J}_{kl}
    \frac{\partial g}{\partial w_l}
\end{align}
and we can immediately go back to the definition of $\hat{J}$ and see
\begin{align}
    \left\{ f, g \right\}
    &=
    \sum_i
    \left( 
    \frac{\partial f}{\partial Q_i}
    \frac{\partial g}{\partial P_i}
    -
    \frac{\partial g}{\partial Q_i}
    \frac{\partial f}{\partial P_i}
    \right)
\end{align}
and we are done.

So the Poisson bracket can be evaluated in any coordinate system,
provided they are related by canonical transformation.
It's a huge subject.
There is a chapter in Goldstein dedicated to canonical transformation.
The current edition is quite a bit better than the earlier ones.
They are mostly taken from David Tong's lectures.
But,
you can find this in Goldstein as well.
Not in the first edition,
but it is in the second edition.

Now I'm going to start a new topic.

\section{Action-Angle Variables}
Consider a one-dimensional system $H(q, p)$
without explicit time-dependence.
So then the total energy is conserved.
We assume that the motion is bounded,
so there exists some $q_1$, $q_2$ such that
\begin{align}
    q_1 \le q \le q_2
\end{align}
So then imagine an arbitrary potential.
The system undergoes periodic motion,
with turning points $q_1$ and $q_2$.
Transform from
$(p, q)$
to new variables
$(I, \theta)$
which have the property that $H$ is independent of $\theta$,
so the Hamiltonian is a function of $I$
\begin{align}
    H = H(I).
\end{align}
In these variables,
from Hamilton's equations
\begin{align}
    -\frac{dI}{dt} &= \frac{\partial H}{\partial \theta} = 0
\end{align}
which means that $I$ is a constant of the motion.
Then what about the other Hamilton's equation?
\begin{align}
    \frac{d\theta}{dt} &=
    \frac{\partial H}{\partial I}
\end{align}
which is some function of $I$.
Remember $H$ does not contain $t$ and it does not contain $\theta$,
and $I$ we just showed to be a constant.
So this is also a constant.

So the equations of motion are very simple.
So this equation can be integrated to find that $\theta$ is just a constant
times time,
and the first equation just tells you $I$ is a constant.

And so by convention,
$I$ and $\theta$ are normalized such that
$\dot{\theta} = \omega$,
where $\omega$ is the angular frequency of this oscillation.

$I$ is called the \emph{action variable}
and $\theta$ is called the \emph{angle variable}.

So $I$ is like the momentum and $\theta$ is like the position.

The point is that in terms of the action-angle variables,
the problem is very simple.
This equation just tells you $I$ is a constant,
and this equation tells you this can be integrated to $\theta=\omega t$.
That's easy.

The hard work is figuring out what is the relation between $I$ and $\theta$.
If you can find the action-angle variables,
you can trivially solve the problem.
The challenge is how do you find the action-angle variables.
If you can find those variables,
you basically solve the problem.

I'm going to show you how to do his for a special case and then we'll talk
about how potentially generalize it.

Almost all the solve problems there are,
you can reduce it down to action-angle variables,
the Kepler problem,
the harmonic oscillator,
etc.
But there are chaotic systems for which there is no such variable.s
But in more complicated systems,
you usually start from a limit that does have action-angle variables,
and then you perturb.
So you might not have exact action-angle variables,
but then you start from a point where you do.
In this case,
they exist because there is a constant of motion,
the conserved energy,
and you will see it is related in a trivial way to the action variable.
Once you have an action variable,
that's all you need.
If you have an $n$-dimensional system with conserved energy,
there is a theorem which says you do have action-angle variables.
But if energy is not conserved,
you may not have action-angle variables in general.

Consider
\begin{align}
    H &=
    \frac{p^2}{2m} + V(q)
\end{align}
If $I$ is a constant of the motion,
it must be some function of the energy.
\begin{align}
    H &= H(I) =  E
\end{align}
Then
\begin{align}
    \dot{\theta}
    &=
    \frac{\partial H}{\partial I}
    =
    \frac{d E}{dI}
    = \omega
\end{align}
This is because $H$ does not depend on $\theta$.
Remember that we normalized $\dot{\theta}$
and this $\omega$ is the angular frequency of oscillation.
That's because we normalized the action-angle variables to get this.
So this is the setting.
For this Hamiltonian,
we have to find the action variable which is a constant of the motion,
some function of the energy,
and it has this property that $\frac{dE}{dI} = \omega$.

The claims is the following.
The correct choice of $I$ is
\begin{align}
    I &=
    \frac{1}{2\pi}
    \oint p\, dq
\end{align}
where this is the area in phase space enclsoed by once orbit.

In other words,
in physical space,
this thing is just bouncing back and forth between $q_1$ and $q_2$.
Let's thing about what this means in phase space.

There is a turning point $q_2$ and there's a turning point $q_1$.
And the horizontal axis is $q$ and the vertical axis is $p$.
The system traces out an orbit in phase space.
This has a reflection symmetry about the $q$ axis.
This is because therei s a positive value of $p$ and it comes back with a
negative value of $p$,
since the root of the equation gives two solutions.
So this thing is the enclosed area,
which is obviously a function of the energy.
With more energy you can go further apart.
So this is a function of the energy and this gives a relation between the energy
and $I$.
This is just a claim we haven't proved it yet.

Okay,
so let's now try to prove this claim.
To prove this,
we need to show that
\begin{align}
    \frac{d}{dE}\left( 
    \oint p\, dq
    \right)
    =
    \frac{2\pi}{\omega}
\end{align}
where $\omega$ is the frequency of oscillation.
Alright, so this is what we're going to show.
So now we have a bit of work.
How do we evalaute the area of this curve for a given $E$?
\begin{align}
    p &=
    \sqrt{2m\left( E - V(q) \right)}
\end{align}
As $E$ is changed, two things happen.
Firstly,
obviously the value of $p$ at every point $q$ is altered.
So
\begin{align}
    p &=
    p
    +
    \left( \frac{\partial p}{\partial E}\right)_q
    \Delta E
\end{align}
By convention the square root is always positive.
So if we change the energy,
at every point inside this integral,
the valueof $E$ is going to change for a given $q$.

Secondly,
the end points $q_1$ and $q_2$ are shifted.
That's because if you change $E$,
your $q_1$ and $q_2$ are not the same now.
We're going to see how much this changes if I change $E$.
That's what we're planning to do.

The value of that integral changes because $E$ is changing at every point.
And the end points are shifting.
We're going to account for both these effects.
